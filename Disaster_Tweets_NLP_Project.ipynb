{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f844b042",
   "metadata": {},
   "source": [
    "# Disaster Tweets NLP Classification — Mini Project\n",
    "Welcome to this notebook for the Kaggle NLP Disaster Tweets mini-project. We will explore, build, and evaluate a model that classifies tweets as disaster-related or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b1b18",
   "metadata": {},
   "source": [
    "## 1. Problem Description\n",
    "Twitter is widely used to announce emergencies and disasters in real time. However, not all tweets containing disaster-related words actually describe a real disaster. The goal of this project is to build a machine learning model that can classify whether a tweet is about a real disaster (target = 1) or not (target = 0).\n",
    "\n",
    "The dataset contains about 10,000 tweets with labels indicating if they describe a real disaster. This is a binary text classification problem in Natural Language Processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9f5f46",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "### 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "print(\"Train dataset shape:\", train.shape)\n",
    "print(\"Test dataset shape:\", test.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51187f9",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Overview\n",
    "- `id`: Unique identifier for each tweet\n",
    "- `keyword`: Important keyword from the tweet (can be NaN)\n",
    "- `location`: Location of the tweet (can be NaN)\n",
    "- `text`: The tweet text (our input feature)\n",
    "- `target`: 1 if tweet is about a real disaster, else 0 (our label)\n",
    "\n",
    "Check label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dba5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='target', data=train)\n",
    "plt.title('Distribution of Disaster (1) vs Non-Disaster (0) Tweets')\n",
    "plt.show()\n",
    "\n",
    "print(train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c41af3",
   "metadata": {},
   "source": [
    "### 2.3 Data Cleaning\n",
    "Tweets often contain URLs, mentions, hashtags, emojis, and punctuation that can add noise. We clean the text by:\n",
    "\n",
    "- Lowercasing\n",
    "- Removing URLs, mentions (@user), hashtags (#tag)\n",
    "- Removing punctuation and digits\n",
    "- Removing extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e78f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)     # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)        # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)        # Remove hashtags\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)    # Remove punctuation and digits\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()# Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "train['clean_text'] = train['text'].apply(clean_text)\n",
    "test['clean_text'] = test['text'].apply(clean_text)\n",
    "\n",
    "train[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe80595",
   "metadata": {},
   "source": [
    "### 2.4 Word Cloud Visualization\n",
    "Visualize common words in disaster vs non-disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35781c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Disaster tweets word cloud\n",
    "disaster_text = \" \".join(train[train['target'] == 1]['clean_text'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(disaster_text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Common Words in Disaster Tweets')\n",
    "plt.show()\n",
    "\n",
    "# Non-disaster tweets word cloud\n",
    "non_disaster_text = \" \".join(train[train['target'] == 0]['clean_text'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(non_disaster_text)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Common Words in Non-Disaster Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c25724",
   "metadata": {},
   "source": [
    "## 3. Model Architecture and Approach\n",
    "We use a neural network based on word embeddings + bidirectional LSTM to classify tweets.\n",
    "\n",
    "- Embeddings capture semantic meaning of words.\n",
    "- Bidirectional LSTM captures sequence context.\n",
    "- Dense sigmoid layer outputs disaster probability.\n",
    "\n",
    "This architecture suits the sequential nature of text and helps learn context in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdf4e0",
   "metadata": {},
   "source": [
    "### 3.1 Text Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f834d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_vocab_size = 10000\n",
    "max_seq_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train['clean_text'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train['clean_text'])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1b9dae",
   "metadata": {},
   "source": [
    "### 3.2 Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train_pad, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training samples:\", X_tr.shape[0])\n",
    "print(\"Validation samples:\", X_val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43534563",
   "metadata": {},
   "source": [
    "### 3.3 Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e233fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=max_seq_length),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcac5b8",
   "metadata": {},
   "source": [
    "### 3.4 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addfae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d527a0e",
   "metadata": {},
   "source": [
    "## 4. Results and Analysis\n",
    "### 4.1 Training History Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over epochs')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6bc61e",
   "metadata": {},
   "source": [
    "### 4.2 Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da689159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "y_val_pred_prob = model.predict(X_val)\n",
    "y_val_pred = (y_val_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"Validation F1 score:\", f1_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3b040",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "- The bidirectional LSTM model performed reasonably well for disaster tweet classification.\n",
    "- Cleaning tweets helped reduce noise.\n",
    "- Word embeddings helped capture semantics.\n",
    "- Adding dropout helped with regularization.\n",
    "- Future improvements: pretrained embeddings, transformer models, hyperparameter tuning, including metadata features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ed4d4",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "- [Kaggle NLP Disaster Tweets competition](https://www.kaggle.com/c/nlp-getting-started)\n",
    "- Chollet, François. *Deep Learning with Python*. Manning, 2018.\n",
    "- TensorFlow Keras Documentation: https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "- WordCloud Python package documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad404226",
   "metadata": {},
   "source": [
    "## 7. Submission File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b27372",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(test['clean_text'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "test_pred_prob = model.predict(X_test_pad)\n",
    "test_pred = (test_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "submission = pd.DataFrame({'id': test['id'], 'target': test_pred})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
